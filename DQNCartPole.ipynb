{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzP6pZaARmZ1egIMcZsqdn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaron020/CNN_Project/blob/main/DQNCartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uw7ADFLK6Yx"
      },
      "outputs": [],
      "source": [
        "#https://www.youtube.com/watch?v=OYhFoMySoVs\n",
        "\n",
        "\n",
        "#Import Dependencies \n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import os # for creating directories\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1') # initialise environment"
      ],
      "metadata": {
        "id": "OBybiPk2MTOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e7f557-67e8-4718-b61d-c94a65aaea9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4 possible states in cartpole \n",
        "state_size = env.observation_space.shape[0]\n",
        "state_size\n",
        "\n",
        "#2 actions in cartpole -> left and right \n",
        "action_size = env.action_space.n\n",
        "action_size\n",
        "\n",
        "batch_size = 32 \n",
        "\n",
        "n_episodes = 5\n",
        "\n",
        "output_dir = 'model_output/cartpole/'"
      ],
      "metadata": {
        "id": "pJ7Co3qBMZYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ],
      "metadata": {
        "id": "sokxbzL4MnWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Agent"
      ],
      "metadata": {
        "id": "y3477SoWPlSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000) # double-ended queue; acts like list, but elements can be added/removed from either end\n",
        "        self.gamma = 0.95 # decay or discount rate: enables agent to take into account future actions in addition to the immediate ones, but discounted at this rate\n",
        "        self.epsilon = 1.0 # exploration rate: how much to act randomly; more initially than later due to epsilon decay\n",
        "        self.epsilon_decay = 0.975 # decrease number of random explorations as the agent's performance (hopefully) improves over time\n",
        "        self.epsilon_min = 0.01 # minimum amount of random exploration permitted\n",
        "        self.learning_rate = 0.001 # rate at which NN adjusts models parameters via SGD to reduce cost \n",
        "        self.model = self._build_model() # private method \n",
        "    \n",
        "    def _build_model(self):\n",
        "        # neural net to approximate Q-value function:\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu')) # 1st hidden layer; states as input\n",
        "        model.add(Dense(24, activation='relu')) # 2nd hidden layer\n",
        "        model.add(Dense(self.action_size, activation='linear')) # 2 actions, so 2 output neurons: 0 and 1 (L/R)\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done)) # list of previous experiences, enabling re-training later\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon: # if acting randomly, take random action\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state, verbose=0) # if not acting randomly, predict reward value based on current state\n",
        "        return np.argmax(act_values[0]) # pick the action that will give the highest reward (i.e., go left or right?)\n",
        "\n",
        "    def replay(self, batch_size): # method that trains NN with experiences sampled from memory\n",
        "        minibatch = random.sample(self.memory, batch_size) # sample a minibatch from memory\n",
        "        for state, action, reward, next_state, done in minibatch: # extract data for each minibatch sample\n",
        "            target = reward # if done (boolean whether game ended or not, i.e., whether final state or not), then target = reward\n",
        "            if not done: # if not done, then predict future discounted reward\n",
        "                target = (reward + self.gamma * # (target) = reward + (discount rate gamma) * \n",
        "                          np.amax(self.model.predict(next_state, verbose=0)[0])) # (maximum target Q based on future action a')\n",
        "            target_f = self.model.predict(state, verbose=0) # approximately map current state to future discounted reward\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0) # single epoch of training with x=state, y=target_f; fit decreases loss btwn target_f and y_hat\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "metadata": {
        "id": "_IE09Q6CMo5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(state_size, action_size) # initialise agent"
      ],
      "metadata": {
        "id": "nzTPt0GYMtmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.decomposition import PCA\n",
        "done = False\n",
        "episodes = []\n",
        "scores = []\n",
        "epsilon = []\n",
        "\n",
        "for e in range(n_episodes): # iterate over new episodes of the game\n",
        "    state = env.reset() # reset state at start of each new episode of the game\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    #pca = PCA(n_components=2) # initialize PCA with 2 components (reduce state space to 2 dimensions)\n",
        "    #pca.fit(state) # fit PCA on the current state\n",
        "    #state = pca.transform(state) \n",
        "    \n",
        "    for time in range(5000):  # time represents a frame of the game; goal is to keep pole upright as long as possible up to range, e.g., 500 or 5000 timesteps\n",
        "#         env.render()\n",
        "        action = agent.act(state) # action is either 0 or 1 (move cart left or right); decide on one or other here\n",
        "        next_state, reward, done, _ = env.step(action) # agent interacts with env, gets feedback; 4 state data points, e.g., pole angle, cart position        \n",
        "        reward = reward if not done else -10 # reward +1 for each additional frame with pole upright        \n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.remember(state, action, reward, next_state, done) # remember the previous timestep's state, actions, reward, etc.        \n",
        "        state = next_state # set \"current state\" for upcoming iteration to the current next state        \n",
        "        if done: # episode ends if agent drops pole or we reach timestep 5000\n",
        "            print(\"episode: {}/{}, score: {}, e: {:.2}\" # print the episode's score and agent's epsilon\n",
        "                  .format(e, n_episodes, time, agent.epsilon))\n",
        "            episodes.append(e)\n",
        "            scores.append(time)\n",
        "            break # exit loop\n",
        "    if len(agent.memory) > batch_size:\n",
        "        agent.replay(batch_size) # train the agent by replaying the experiences of the episode\n",
        "    if e % 50 == 0:\n",
        "        agent.save(output_dir + \"weights_\" + '{:04d}'.format(e) + \".hdf5\")         "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "A2jguDi_Mv7s",
        "outputId": "10930f3e-a4cb-4eaa-9448-2737a1fb90c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-beed6879d711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#state = np.reshape(state, [1, state_size])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# initialize PCA with 2 components (reduce state space to 2 dimensions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fit PCA on the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \"\"\"\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    428\u001b[0m             )\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    770\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 0.03732438 -0.00496428  0.01570378 -0.03373716].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WeBehuowjhaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(episodes,scores, label=\"Scores\")\n",
        "plt.legend(loc=2)\n",
        "\n",
        "plt.savefig(\"scores_png.png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ge8SAlmLjiKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_score = []\n",
        "average_ep = []\n",
        "average = 0\n",
        "for x in range(len(scores)):\n",
        "  average += scores[x]\n",
        "  if x % 10 == 0:\n",
        "    average_score.append(average/10)\n",
        "    average_ep.append(x)\n",
        "    average = 0\n",
        "print(average_score)\n",
        "print(average_ep)"
      ],
      "metadata": {
        "id": "AhVmZHbiymeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_percentage = []\n",
        "epsilon_value = 1.0\n",
        "for x in range(len(episodes)):\n",
        "  epsilon_percentage.append(epsilon_value)\n",
        "  epsilon_value *= .975\n",
        "\n",
        "print(epsilon_percentage)\n",
        "\n",
        "for x in range(len(episodes)):\n",
        "  epsilon_percentage[x] = epsilon_percentage[x] * 100\n",
        "\n",
        "print(epsilon_percentage)"
      ],
      "metadata": {
        "id": "Ka6zh9NA0Lc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(episodes,scores, label=\"Scores\", alpha=0.5)\n",
        "plt.plot(average_ep,average_score, label=\"Average Scores\")\n",
        "plt.plot(episodes, epsilon_percentage, label=\"Epsilon Percentage\")\n",
        "plt.legend(loc=2)\n",
        "#plt.xlim([0, 250])\n",
        "#plt.ylim([0,200])\n",
        "\n",
        "plt.savefig(\"epsilon.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "aZ3La22fzkFi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}