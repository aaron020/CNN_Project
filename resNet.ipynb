{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1T5BV/aDi6e0xtLCNoPtV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaron020/CNN_Project/blob/main/resNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z7_JVyK95v6M"
      },
      "outputs": [],
      "source": [
        "#Based on tutorial from : https://www.youtube.com/watch?v=glmowUlqoYw\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pull in a dataset from your computer\n",
        "pathName = \"Shoe vs Sandal vs Boot Dataset\"\n",
        "\n",
        "\n",
        "footwear_types = os.listdir(pathName)\n",
        "\n",
        "print(\"The types of footwear are = \" , footwear_types)"
      ],
      "metadata": {
        "id": "DL7RQtOB58fS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516ec1a9-9479-4033-dd79-4c3c270c2037"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The types of footwear are =  ['Shoe', 'Sandal', 'Boot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tack all the data from the datset \n",
        "footwear = []\n",
        "\n",
        "for item in footwear_types:\n",
        "  # get all the files from each folder \n",
        "  all_footwear = os.listdir(pathName + '/' + item)\n",
        "\n",
        "  for fw in all_footwear:\n",
        "    footwear.append((item, str(pathName + '/' + item) + '/' + fw))\n",
        "\n",
        "print(footwear[0])"
      ],
      "metadata": {
        "id": "U8F9hHHi6sZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90eccf73-85e9-4894-ccb9-3b4577867528"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Shoe', 'Shoe vs Sandal vs Boot Dataset/Shoe/Shoe (4002).jpg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Store the folder name in a seperate column - the folder name is what the image is\n",
        "\n",
        "#Build a dataframe using pandas\n",
        "\n",
        "footwear_df = pd.DataFrame(data=footwear, columns=['footwear type', 'image'])\n",
        "\n",
        "print(footwear_df.head())"
      ],
      "metadata": {
        "id": "xEI4_J9Q8BYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6b1f67-daf6-41c5-c99e-aab07ea0406d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  footwear type                                              image\n",
            "0          Shoe  Shoe vs Sandal vs Boot Dataset/Shoe/Shoe (4002...\n",
            "1          Shoe  Shoe vs Sandal vs Boot Dataset/Shoe/Shoe (388)...\n",
            "2          Shoe  Shoe vs Sandal vs Boot Dataset/Shoe/Shoe (4868...\n",
            "3          Shoe  Shoe vs Sandal vs Boot Dataset/Shoe/Shoe (3740...\n",
            "4          Shoe  Shoe vs Sandal vs Boot Dataset/Shoe/Shoe (1919...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print info about the dataset\n",
        "\n",
        "print('Total number of rooms in the dataset: ', len(footwear))\n",
        "\n",
        "footwear_count = footwear_df['footwear type'].value_counts()\n",
        "\n",
        "print('Rooms in each category')\n",
        "print(footwear_count)"
      ],
      "metadata": {
        "id": "pJnR8eEz8wlP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c967c72b-f574-4a0a-e085-d8fe7d7c071d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rooms in the dataset:  15000\n",
            "Rooms in each category\n",
            "Shoe      5000\n",
            "Sandal    5000\n",
            "Boot      5000\n",
            "Name: footwear type, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "path = 'Shoe vs Sandal vs Boot Dataset/'\n",
        "\n",
        "#Original image size = 136x102, now we resize to 244x244\n",
        "\n",
        "img_size = 244\n",
        "\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for i in footwear_types:\n",
        "  data_path = path + str(i) #enter into first folder and second and then third \n",
        "  filenames = [i for i in os.listdir(data_path)] # name of each image \n",
        "\n",
        "  for f in filenames:\n",
        "    img = cv2.imread(data_path + '/' + f)\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "    images.append(img)\n",
        "    labels.append(i)"
      ],
      "metadata": {
        "id": "loLc7XLm9lwk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform the image array to a numpy type \n",
        "images = np.array(images)\n",
        "images.shape \n",
        "#Shape ->  total No. of images x size x dimensions (RGB)"
      ],
      "metadata": {
        "id": "Uwb2Hqt2_hVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece685db-0a38-4cc4-bc85-3988d59bcdfe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 244, 244, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = images.astype(np.float16) / 255.0\n",
        "#np.float16 or it will run out of ram\n",
        "#ie divide all the pixel values by 255 just to make the pixel value smaller \n",
        "#255 = max pixel value everythng else is gonna be like 0.13 0.43"
      ],
      "metadata": {
        "id": "iCsGisFV_4Ei"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The labels are in the form of numbers, the computer does not understand that so we convert \n",
        "#them to numbers\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder \n",
        "y = footwear_df['footwear type'].values\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "onehotencoder = OneHotEncoder()\n",
        "Y = onehotencoder.fit_transform(y)\n",
        "print(Y.shape)\n",
        "print(Y.toarray())\n",
        "\n"
      ],
      "metadata": {
        "id": "h903dWWTAglG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db203a0b-ffce-4e7c-88ef-2756746328f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15000, 3)\n",
            "[[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " ...\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide datset into train and test\n",
        "\n",
        "from sklearn.utils import shuffle \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "images, Y = shuffle(images, Y, random_state=1)\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(images, Y, test_size=0.10, random_state=415)\n",
        "#Very small test set as the dataset is very small\n",
        "\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "niI5ksqECFxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a41cb7-5a26-401a-d3b7-8abe56e6a45c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13500, 244, 244, 3)\n",
            "(13500, 3)\n",
            "(1500, 244, 244, 3)\n",
            "(1500, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Start of building RESNET functions\n",
        "\n",
        "\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image \n",
        "from keras.utils import layer_utils \n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot \n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from keras.initializers import glorot_uniform \n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n"
      ],
      "metadata": {
        "id": "RIwF9miOEymX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_block (X, f, filters):\n",
        "  F1, F2, F3 = filters\n",
        "\n",
        "  #Input value is saved so that it can be applied after conv layers\n",
        "  X_shortcut = X\n",
        "\n",
        "\n",
        "  #First layer                                                                  X is the input \n",
        "  X = Conv2D(filters = F1, kernal_size= (1,1), strides= (1,1), padding = 'same')(X)\n",
        "\n",
        "  #X is the input to batch normalization layer \n",
        "  X = BatchNormalization(axis = 3)(X)\n",
        "\n",
        "  #Relu is the activation function used \n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "\n",
        "\n",
        "  #Secound layer \n",
        "  X = Conv2D(filters = F2, kernal_size= (f,f), strides= (1,1), padding = 'same')(X)\n",
        "  X = BatchNormalization(axis = 3)(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  #Third layer \n",
        "  X = Conv2D(filters = F3, kernal_size= (1,1), strides= (1,1), padding = 'valid')(X)\n",
        "  X = BatchNormalization(axis = 3)(X)\n",
        "  #No relu here instead we push it to be added to the first input\n",
        "\n",
        "  #Add the shortcut value to F(x)\n",
        "  X = Add()([X, X_shortcut])\n",
        "  X = Activation('relu')(X) #Relu is ued because it will transform all negative pixel vals to 0\n",
        "\n",
        "  return X\n"
      ],
      "metadata": {
        "id": "2NC7hKx_G8Vy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convolutional_block(X,f,filters, s=2):\n",
        "  F1, F2, F3 = filters\n",
        "\n",
        "  X_shortcut = X\n",
        "\n",
        "  #First layer - stride causes a reduction in the size of the image\n",
        "  X = Conv2D(filters = F1, kernal_size = (1,1), strides = (s,s)) (X)\n",
        "  X = BatchNormalization(axis = 3)(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  #Second layer\n",
        "  X = Conv2D(filters = F2, kernal_size= (1,1), strides = (1,1), padding = 'same') (X)\n",
        "  X = BatchNormalization(axis = 3)(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  \n",
        "  #Third layer\n",
        "  X = Conv2D(filters = F3, kernal_size= (1,1), strides = (1,1), padding = 'valid')(X)\n",
        "  X = BatchNormalization(axis = 3)(X)\n",
        "\n",
        "  #The output now has a different size to the input \n",
        "  #Thus we must make a change to the shoutcut\n",
        "  #Notice this conv layer is almost the same as the first layer (the one that reduced the size)\n",
        "  X_shortcut = Conv2D(filters = F3, kernal_size= (1,1), strides = (s,s), padding = 'valid')(X_shortcut)\n",
        "  X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n",
        "\n",
        "  X = add()([X, X_shortcut])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "OA02lFwf9_bo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resnet algorithm  - all 50 layers\n",
        "\n",
        "\n",
        "def Resnet50(input_shape=(224,224,3), classes=3):\n",
        "\n",
        "  #Define the input with shape input_shape \n",
        "  X_input = Input(input_shape)\n",
        "\n",
        "  #zero padding \n",
        "  X = ZeroPadding2D((3,3))(X_input) #3x3 padding, thus increasing the size \n",
        "  #becaue the size of the image can become too small without the padding \n",
        "  #Allows use to preserve the information - optional \n",
        "\n",
        "  #Stage 1\n",
        "  X = Conv2D(64, (7,7), strides=(2,2))(X)\n",
        "  X = BatchNormalization(axis = 3)(X)\n",
        "  X = Activation('relu')(X)\n",
        "  X = MaxPooling2D((3,3), strides=(2,2))(X)\n",
        "\n",
        "\n",
        "  #Stage 2\n",
        "  X = convolutional_block(X, f=3, filters=[64,64,256], s=1)\n",
        "\n",
        "  X = identity_block(X, 3, [64,64,256])\n",
        "\n",
        "  X = identity_block(X, 3, [64,64,256])\n",
        "\n",
        "\n",
        "  #Stage 3\n",
        "  X = convolutional_block(X, f=3, filters=[128,128,512], s=2)\n",
        "\n",
        "  X = identity_block(X, 3, [128,128,512])  \n",
        "\n",
        "  X = identity_block(X, 3, [128,128,512])  \n",
        "\n",
        "  X = identity_block(X, 3, [128,128,512])  \n",
        "\n",
        "  #Stage 4\n",
        "  X = convolutional_block(X, f=3, filters=[256, 256, 1024], s=2)\n",
        "\n",
        "  X = identity_block(X, 3, [256,256,1024])\n",
        "\n",
        "  X = identity_block(X, 3, [256,256,1024])\n",
        "\n",
        "  X = identity_block(X, 3, [256,256,1024])\n",
        "\n",
        "  X = identity_block(X, 3, [256,256,1024])\n",
        "\n",
        "  X = identity_block(X, 3, [256,256,1024])\n",
        "\n",
        "  #Stage 5\n",
        "  X = convolutional_block(X, f=3, filters=[512, 512, 2048], s=2)\n",
        "\n",
        "  X = identity_block(X, 3, [512,512,2048])\n",
        "\n",
        "  X = identity_block(X, 3, [512,512,2048])\n",
        "\n",
        "\n",
        "  #Average pooling layer \n",
        "  X = AveragePooling2D((2,2),name=\"avg_pool\")(X)\n",
        "\n",
        "\n",
        "  #Output layer \n",
        "  #Convert image to a 1 dimensional vector \n",
        "  X = Flatten()(X)\n",
        "  #name = 'fc' means nothing \n",
        "  X = Dense(classes, activation='softmax', name='fc' + str(classes), kernal_initializer = glorot_uniform(seed=0))(X)\n",
        "\n",
        "  #Create the model \n",
        "  model = Model(inputs = X_input, outputs = X, name = 'Resnet50')\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "vfWFYKXJAtgj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Resnet50()"
      ],
      "metadata": {
        "id": "M4NoFNZ9It1C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "5208cc98-6322-418e-fd83-5427fca573c0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6ce4764c330f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-da7e81a563ac>\u001b[0m in \u001b[0;36mResnet50\u001b[0;34m(input_shape, classes)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m#Stage 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolutional_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentity_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-459b4cd1b67c>\u001b[0m in \u001b[0;36mconvolutional_block\u001b[0;34m(X, f, filters, s)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#First layer - stride causes a reduction in the size of the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/dtensor/utils.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m           \u001b[0mlayout_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_layout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0minit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Inject the layout parameter after the invocation of __init__()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'kernel_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jCAyiIMsI8SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "tJnpptKyJInu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_x, train_y, epochs = 3, batch_size = 32)"
      ],
      "metadata": {
        "id": "EM5Hx2S-E3UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_x, test_y)"
      ],
      "metadata": {
        "id": "2z9bHO58FA1j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}